# Backend with two REST APIs using FastAPI :

1. Document Ingestion API(done ready to digest any .pdf or .txt file)
    - Upload .pdf or .txt files(done)
    - Extract text, apply two chunking strategies (selectable)(done)
    - Generate embeddings & store in Pinecone/Qdrant/Weaviate/Milvus (pc connectiondone, storing done)
    - Save metadata in SQL/NoSQL DB (db connection done, storing the metadata done)
2. Conversational RAG API
    - Custom RAG (no RetrievalQAChain)(done)
    - Use Redis for chat memory(done using redis cloud)
    - Handle multi-turn queries(done)
    - Support interview booking (name, email, date, time) using  LLM(done)
    - Store booking info(done)


### Constraints:
No FAISS/Chroma, no UI, no RetrievalQAChain clean modular code following industry standards for typing and annotations

## ðŸŽ¯ API Overview

### 1. **Document Ingestion API**
- **Endpoint**: `POST /documents/upload/`
- **Purpose**: Upload and process documents (.pdf, .txt)
- **Features**:
  - File upload with validation
  - Text extraction
  - Chunking (Fixed/Semantic strategies)
  - Embedding generation & storage (Pinecone)
  - Metadata storage (Neon PostgreSQL)

### 2. **Conversational RAG API**
- **Endpoint**: `POST /RAG/chat`
- **Purpose**: Chat with AI using knowledge base and memory
- **Features**:
  - Multi-turn conversations (Redis memory)
  - Knowledge base retrieval (optional)
  - Interview booking detection & storage
  - Session management (continue/restart)



## ðŸ”„ Workflow Diagrams

### Document Ingestion Flow:
```
Upload File (.pdf/.txt)
    â†“
Extract Text (PyPDF/UTF-8)
    â†“
Choose Chunking Strategy (Fixed/Semantic)
    â†“
Apply Chunking
    â†“
Generate Embeddings (Pinecone Inference API)
    â†“
Store Vectors (Pinecone - default namespace)
    â†“
Save Metadata (Neon PostgreSQL)
    â†“
Return Response
```

### Conversational RAG Flow:
```
User Query
    â†“
Load History (Redis Cloud)
    â†“
Detect Booking Intent? â”€â”€â†’ YES â†’ Extract Info â†’ Save to DB
    â†“ NO
Retrieve Context (Pinecone) if knowledge_base=yes
    â†“
Generate Response (Groq LLM)
    â†“
Save to History (Redis)
    â†“
Return Response
```



## Status
- project initialization(done)
- uploading file(done)
- acceopting only the supported file type .pdf & .txt(done)
- text extraction using pypdf for .pdf and utf-8 decoding for .txt(done)
- selectable chunking(fixed, semantic) strategy(done)
- apply chunking strategy(done)
- embedding and storing both pinecone directly provides integrated interface for embedding models so we don't need to do embedding by ourself (done)
- saving document metadata in neon postgress(done)


## project architecture
```
RAGtask1/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ endpoints.py             # FastAPI router definitions
â”‚   â”‚   â”œâ”€â”€ models.py                # Pydantic models for API requests/responses
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ db.py                    # Database connection setup (SQL/NoSQL)
â”‚   â”œâ”€â”€ services/
|   |   â”œâ”€â”€ llm_wrapper.py           # custom LLM wrapper (groq)
â”‚   â”‚   â”œâ”€â”€ document_service.py      # Logic for text extraction, chunking, embedding
â”‚   â”‚   â”œâ”€â”€ vector_store_manager.py  # Pinecone/Qdrant connection and interaction
â”‚   â”‚   â”œâ”€â”€ llm_service.py           # Logic for RAG chain, memory, and function calling
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ main.py
```
